{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cria a estrutura de pastas que o código original assume existir"
      ],
      "metadata": {
        "id": "MZcd7qjo-Jys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Define the target folders and create them if they don't exist\n",
        "target_folder_arrays = \"data/arrays\"\n",
        "target_folder_plots = \"data/model/unet/plots\"\n",
        "target_folder_results = \"data/model/unet/results/\"\n",
        "os.makedirs(target_folder_arrays, exist_ok=True)\n",
        "os.makedirs(target_folder_plots, exist_ok=True)\n",
        "os.makedirs(target_folder_results, exist_ok=True)\n",
        "\n",
        "# Zenodo record ID and API endpoint to fetch metadata\n",
        "record_id = \"7189381\"\n",
        "api_url = f\"https://zenodo.org/api/records/{record_id}\"\n",
        "\n",
        "# Fetch the record metadata\n",
        "response = requests.get(api_url)\n",
        "if response.status_code != 200:\n",
        "    print(\"Error: Could not retrieve record metadata from Zenodo.\")\n",
        "else:\n",
        "    record_data = response.json()\n",
        "    if \"files\" not in record_data:\n",
        "        print(\"No files found in this Zenodo record.\")\n",
        "    else:\n",
        "        for file_info in record_data[\"files\"]:\n",
        "            file_name = file_info[\"key\"]\n",
        "\n",
        "            # Try to use 'download' key, or fallback to 'bucket'\n",
        "            download_url = file_info[\"links\"].get(\"self\")\n",
        "\n",
        "            if not download_url:\n",
        "                print(f\"No valid download URL found for {file_name}\")\n",
        "                continue\n",
        "\n",
        "            file_path = os.path.join(target_folder_arrays, file_name)\n",
        "            print(f\"Downloading {file_name} from {download_url}...\")\n",
        "            with requests.get(download_url, stream=True) as r:\n",
        "                r.raise_for_status()\n",
        "                with open(file_path, \"wb\") as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "            print(f\"Downloaded {file_name} to {file_path}\")\n"
      ],
      "metadata": {
        "id": "isrw-mPV7fiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Instala o pacote segmentation-models"
      ],
      "metadata": {
        "id": "ZMFE33EL-WFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models"
      ],
      "metadata": {
        "id": "NC8qdZkj8NMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFa5ot3uT106"
      },
      "source": [
        "# Train U-Net on the HR Global Landslide Detection Dataset\n",
        "\n",
        "*Authors: Sansar Raj Meena, Lorenzo Nava, Kushanav Bhuyan, and Lucas Soares*\n",
        "\n",
        "OBS: ajustes para o segmentation-models funcionar na ultima versão do keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "m1C9Rvd_T107"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# https://github.com/qubvel/segmentation_models/issues/570#issuecomment-1641512983\n",
        "# solucão para o segmentation_models desatualizado\n",
        "import os\n",
        "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow.python.keras.utils.generic_utils as generic_utils\n",
        "#from tensorflow.python.keras.utils import generic_utils\n",
        "tf.keras.utils.generic_utils = generic_utils\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# from losses import dice_loss\n",
        "def dsc(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dsc(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Library with segmentation metrics\n",
        "import segmentation_models as sm\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "for device in physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "\n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "\n",
        "else:\n",
        "\n",
        "   print(\"Please install GPU version of TF\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Xl5lhAgKT108"
      },
      "outputs": [],
      "source": [
        "X_train = np.load(f'data/arrays/trainX.npy')\n",
        "Y_train = np.load(f'data/arrays/trainY.npy')\n",
        "X_val = np.load(f'data/arrays/valX.npy')\n",
        "Y_val = np.load(f'data/arrays/valY.npy')\n",
        "X_test = np.load(f'data/arrays/testX.npy')\n",
        "Y_test = np.load(f'data/arrays/testY.npy')\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(Y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "m6hgINq3T108"
      },
      "outputs": [],
      "source": [
        "# Visualise some data\n",
        "for i in range(5):\n",
        "    f, axarr = plt.subplots(1,2,figsize=(8,8))\n",
        "    axarr[0].imshow(X_train[i][:,:,:3])\n",
        "    axarr[1].imshow(np.squeeze(Y_train[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "9BYpsGZwT108"
      },
      "outputs": [],
      "source": [
        "# Define the evaluation metrics - Precision, Recall, FScore, IoU\n",
        "metrics = [sm.metrics.Precision(threshold=0.5),sm.metrics.Recall(threshold=0.5),sm.metrics.FScore(threshold=0.5,beta=1),sm.metrics.IOUScore(threshold=0.5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treina o modelo"
      ],
      "metadata": {
        "id": "OVMLvkeg_H9C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giJX16AFT108"
      },
      "outputs": [],
      "source": [
        "# Model training - Results are saved in a .csv file\n",
        "\n",
        "# Size of the tiles\n",
        "size = X_train.shape[2]\n",
        "# Image bands\n",
        "img_bands = X_train.shape[3]\n",
        "# Sampling method\n",
        "sampling = \"no_overlap\"\n",
        "# Loss function\n",
        "loss=dice_loss\n",
        "\n",
        "# Number of filters\n",
        "filters = [4, 8, 16, 32]\n",
        "# Learning rates\n",
        "lr = [10e-3, 5e-4, 10e-4, 5e-5, 10e-5]\n",
        "# Batch sizes\n",
        "batch_size = [4, 8, 16, 32]\n",
        "# Epochs\n",
        "epochs = 5\n",
        "\n",
        "# Dictionary that will save the results\n",
        "dic = {}\n",
        "\n",
        "# Hyperparameters\n",
        "dic[\"model\"] = []\n",
        "dic[\"batch_size\"] = []\n",
        "dic[\"learning_rate\"] = []\n",
        "dic[\"filters\"] = []\n",
        "\n",
        "# test area 1\n",
        "dic[\"precision_area\"] = []\n",
        "dic[\"recall_area\"] = []\n",
        "dic[\"f1_score_area\"] = []\n",
        "dic[\"iou_score_area\"] = []\n",
        "\n",
        "# loop over all the filters in the filter list\n",
        "for fiilter in filters:\n",
        "    # loop over the learning rates\n",
        "    for learning_rate in lr:\n",
        "        # loop over all batch sizes in batch_size list\n",
        "        for batch in batch_size:\n",
        "            print('_______________________________________________________________________________')\n",
        "            print('Filters: ', fiilter)\n",
        "            print('Learning rate: ', learning_rate)\n",
        "            print('Batch size: ', batch)\n",
        "\n",
        "            def unet(lr,filtersFirstLayer, pretrained_weights = None,input_size = (size,size,img_bands)):\n",
        "                inputs = Input(input_size)\n",
        "                conv1 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(inputs)\n",
        "                conv1 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv1)\n",
        "                pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "                conv2 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool1)\n",
        "                conv2 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv2)\n",
        "                pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "                conv3 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool2)\n",
        "                conv3 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv3)\n",
        "                pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "                conv4 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool3)\n",
        "                conv4 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv4)\n",
        "                pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "                conv5 = Conv2D(filtersFirstLayer*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(pool4)\n",
        "                conv5 = Conv2D(filtersFirstLayer*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv5)\n",
        "\n",
        "                up6 = Conv2D(filtersFirstLayer*8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv5))\n",
        "                merge6 = concatenate([conv4,up6], axis = 3)\n",
        "                conv6 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge6)\n",
        "                conv6 = Conv2D(filtersFirstLayer*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv6)\n",
        "\n",
        "                up7 = Conv2D(filtersFirstLayer*4, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "                merge7 = concatenate([conv3,up7], axis = 3)\n",
        "                conv7 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge7)\n",
        "                conv7 = Conv2D(filtersFirstLayer*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv7)\n",
        "\n",
        "                up8 = Conv2D(filtersFirstLayer*2, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "                merge8 = concatenate([conv2,up8], axis = 3)\n",
        "                conv8 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge8)\n",
        "                conv8 = Conv2D(filtersFirstLayer*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv8)\n",
        "\n",
        "                up9 = Conv2D(filtersFirstLayer, 2, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "                merge9 = concatenate([conv1,up9], axis = 3)\n",
        "                conv9 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(merge9)\n",
        "                conv9 = Conv2D(filtersFirstLayer, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv9)\n",
        "                conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'glorot_normal')(conv9)\n",
        "                conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "                model = Model(inputs, conv10)\n",
        "\n",
        "                model.compile(optimizer = Adam(learning_rate = lr), loss = loss, metrics = metrics)\n",
        "\n",
        "                #model.summary()\n",
        "\n",
        "                if(pretrained_weights):\n",
        "                    model.load_weights(pretrained_weights)\n",
        "\n",
        "                return model\n",
        "\n",
        "            # Load the model\n",
        "            model = unet(filtersFirstLayer= fiilter, lr = learning_rate, input_size = (size,size,img_bands))\n",
        "            # Stop the training if the validation loss does not decrease after 30 epochs\n",
        "            early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss', # what is the metric to measure\n",
        "                              patience = 30, # how many epochs to continue running the model after seeing an increase in val_loss\n",
        "                              restore_best_weights = True) # update the mod\n",
        "            # Save the models only when validation loss decrease\n",
        "            model_checkpoint = tf.keras.callbacks.ModelCheckpoint(f'data/model/unet/weights/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.weights.h5',\n",
        "                                                                  monitor='val_loss', mode='min',verbose=0, save_best_only=True,save_weights_only = True)\n",
        "\n",
        "            # fit the model 20% of the dataset was used as validation\n",
        "            history = model.fit(X_train,Y_train,batch_size = batch,epochs=epochs,validation_split=0.2,\n",
        "                                callbacks = [model_checkpoint, early_stop], verbose=1)\n",
        "\n",
        "            # summarize history for iou score\n",
        "            plt.plot(history.history['f1-score'])\n",
        "            plt.plot(history.history['val_f1-score'])\n",
        "            plt.title('model f1-score')\n",
        "            plt.ylabel('f1-score')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.legend(['train', 'validation'], loc='upper left')\n",
        "            # save plots\n",
        "            plt.savefig(f\"data/model/unet/plots/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_iou_score.png\")\n",
        "            plt.show()\n",
        "            # summarize history for loss\n",
        "            plt.plot(history.history['loss'])\n",
        "            plt.plot(history.history['val_loss'])\n",
        "            plt.title('model loss')\n",
        "            plt.ylabel('loss')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.legend(['train', 'validation'], loc='upper left')\n",
        "            plt.savefig(f\"data/model/unet/plots/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}_val_loss.png\")\n",
        "            plt.show()\n",
        "\n",
        "            # load unet to evaluate the test data\n",
        "            attn_unet = unet(filtersFirstLayer= fiilter, lr = learning_rate,input_size=(size,size,img_bands))\n",
        "            # load the last saved weight from the training\n",
        "            attn_unet.load_weights(f\"data/model/unet/weights/unet_{sampling}_size_{size}_filters_{fiilter}_batch_size_{batch}_lr_{learning_rate}.weights.h5\")\n",
        "\n",
        "           # Evaluate the model\n",
        "            res_1= attn_unet.evaluate(X_test,Y_test)\n",
        "\n",
        "\n",
        "            # save results on the dictionary\n",
        "            dic[\"model\"].append(\"Unet\")\n",
        "            dic[\"batch_size\"].append(batch)\n",
        "            dic[\"learning_rate\"].append(learning_rate)\n",
        "            dic[\"filters\"].append(fiilter)\n",
        "            dic[\"precision_area\"].append(res_1[1])\n",
        "            dic[\"recall_area\"].append(res_1[2])\n",
        "            dic[\"f1_score_area\"].append(res_1[3])\n",
        "            dic[\"iou_score_area\"].append(res_1[4])\n",
        "\n",
        "            # Convert results to a dataframe\n",
        "            results = pd.DataFrame(dic)\n",
        "            # Export as csv\n",
        "            results.to_csv(f'data/model/unet/results/results_Unet.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Roda/testa o modelo treinado"
      ],
      "metadata": {
        "id": "JHTswS5E_Ceh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-0Nj3foET109"
      },
      "outputs": [],
      "source": [
        "# Load the best model based on the best performances on the test set (check CSV file)\n",
        "# Loading the model weights\n",
        "unet_best = unet(filtersFirstLayer= 4,lr = 0.0005,input_size=(size,size,img_bands))\n",
        "unet_best.load_weights(\"data/model/unet/weights/unet_no_overlap_size_128_filters_4_batch_size_4_lr_0.01.weights.h5\")\n",
        "\n",
        "# Plot predictions on test set\n",
        "for i in range(X_test.shape[0]):\n",
        "    preds_train_1 = unet_best.predict(np.expand_dims(X_test[i],axis = 0), verbose=0)\n",
        "    # It's possible to change the 0.5 threshold to improve the results;\n",
        "    preds_train_t1 = (preds_train_1 > 0.5).astype(np.uint8)\n",
        "    f, axarr = plt.subplots(1,3,figsize=(10,10))\n",
        "    axarr[0].imshow(X_test[i][:,:,:3])\n",
        "    axarr[1].imshow(np.squeeze(preds_train_t1))\n",
        "    axarr[2].imshow(np.squeeze(Y_test[i]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "c5906200580bdc572e1d260bc7d196a2bb4943342f05a4580bbfcb76fbea5878"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}